{
  "InputLayer": {
    "title": "Input Layer",
    "visual": "Images/InputLayer",
    "description": [
      "The input layer is the network’s entry point, where raw data enters.",
      "Each node takes in one feature - such as weight, durability, or material type - and hands it off to the next layer."
    ],
    "highlights": [
      "One node per feature",
      "No computation - just data intake",
      "Feeds raw values into the network",
      "Sets the foundation for all later processing"
    ]
  },
  "HiddenLayers": {
    "title": "Hidden Layers",
    "visual": "Images/HiddenLayers",
    "description": [
      "Hidden layers sit between the input and output and perform the network’s “thinking” by extracting patterns from data.",
      "Each node applies weights and an activation function, passing its result on until the final output layer makes a decision."
    ],
    "highlights": [
      "Stacked layers let the network learn increasingly abstract features",
      "More layers can capture complex relationships - but risk overfitting if overused",
      "Each node transforms its inputs via weighted sums and activations",
      "Choosing the right layer count is key to balancing accuracy and generalization"
    ]
  },
  "OutputLayer": {
    "title": "Output Layer",
    "visual": "Images/OutputLayer",
    "description": [
      "The output layer turns the final activations into your network’s predictions.",
      "Each node represents one category, and the highest value indicates the chosen class."
    ],
    "highlights": [
      "Number of nodes = number of classes",
      "Commonly uses softmax to produce probabilities",
      "Pick the node with the highest output as the decision",
      "Outputs can be interpreted as confidence scores"
    ]
  },
  "TrainingCycle": {
    "title": "Training Cycles",
    "visual": "Images/TrainingCycles",
    "description": [
      "A training cycle (or epoch) means running the entire dataset through the network once.",
      "More cycles can help the network learn better - but too many can lead to overfitting."
    ],
    "highlights": [
      "One cycle = one full pass over all training samples",
      "Increasing cycles usually lowers loss but may overfit",
      "Watch evaluation metrics to find the sweet spot",
      "Longer training takes more time and compute"
    ]
  },
  "LearningRate": {
    "title": "Learning Rate",
    "visual": "Images/LearningRate",
    "description": [
      "The learning rate controls how much the network’s weights adjust on each update.",
      "A higher rate speeds up learning but can overshoot the optimum; a lower rate is more precise but slower."
    ],
    "highlights": [
      "Too high → training may diverge or oscillate",
      "Too low → training can stall or take too long",
      "Common starting values range from 0.001 to 0.1",
      "Adjust based on loss curve behavior"
    ]
  }
}
